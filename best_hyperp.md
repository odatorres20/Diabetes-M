Tuning hyperparameters for Logistic Regression...
Best Score: 0.9999, Best Params: {'C': 7.158728631500198, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}

Tuning hyperparameters for Random Forest...
Best Score: 1.0000, Best Params: {'bootstrap': False, 'class_weight': 'balanced', 'max_depth': 29, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 812}

Tuning hyperparameters for XGBoost...
Best Score: 1.0000, Best Params: {'colsample_bytree': 0.7544410551203097, 'gamma': 4.474196712982528, 'learning_rate': 0.1748490665654785, 'max_depth': 4, 'min_child_weight': 0.1308848552191608, 'n_estimators': 1053, 'reg_alpha': 0.014030106275654474, 'reg_lambda': 0.016351814261589406, 'subsample': 0.7275902521175045}

Logistic Regression: Best Score = 0.9999, Best Params = {'C': 7.158728631500198, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}
Random Forest: Best Score = 1.0000, Best Params = {'bootstrap': False, 'class_weight': 'balanced', 'max_depth': 29, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 812}
XGBoost: Best Score = 1.0000, Best Params = {'colsample_bytree': 0.7544410551203097, 'gamma': 4.474196712982528, 'learning_rate': 0.1748490665654785, 'max_depth': 4, 'min_child_weight': 0.1308848552191608, 'n_estimators': 1053, 'reg_alpha': 0.014030106275654474, 'reg_lambda': 0.016351814261589406, 'subsample': 0.7275902521175045}