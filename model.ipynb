{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74c876e",
   "metadata": {},
   "source": [
    "## Data preparation for running my model \n",
    "\n",
    "- We handled the missing values and formating of age, weight and race to make it more suitable for the model\n",
    "- We dropped irrelevant and highly missing values variables, we will also drop encounter id and patient number since it do not contribute significantly in anything for the model \n",
    "- Categorical values will be further one-hot encoding into numerical format\n",
    "- Numerical will be standarized for zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3e307d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (81412, 53)\n",
      "Test data shape: (20354, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test sets prepared earlier\n",
    "diabetes_train = pd.read_parquet(\"data/02_train.parquet\")\n",
    "diabetes_test = pd.read_parquet(\"data/02_test.parquet\")\n",
    "\n",
    "print(\"Train data shape:\", diabetes_train.shape)\n",
    "print(\"Test data shape:\", diabetes_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9d2fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from data_eval import map_age_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4d6e5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "class WeightGrouper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col=\"weight\", new_col=\"weight_group\", verbose=False):\n",
    "        self.col = col\n",
    "        self.new_col = new_col\n",
    "        self.verbose = verbose\n",
    "        # mapping lives inside the object\n",
    "        self._weight_map = {\n",
    "            '[0-25)': 'Underweight', '[25-50)': 'Underweight',\n",
    "            '[50-75)': 'Normal', '[75-100)': 'Normal',\n",
    "            '[100-125)': 'Overweight', '[125-150)': 'Obese', '[150-175)': 'Obese',\n",
    "            '[175-200)': 'Severely Obese', '[200-225)': 'Severely Obese',\n",
    "            '[225-250)': 'Morbidly Obese', '[250-275)': 'Morbidly Obese',\n",
    "            '[275-300)': 'Morbidly Obese', '[300-325)': 'Morbidly Obese',\n",
    "            '[325-350)': 'Morbidly Obese', '[350-375)': 'Morbidly Obese',\n",
    "            '[375-400)': 'Morbidly Obese', 'Over 400': 'Morbidly Obese',\n",
    "            '?': 'Normal'\n",
    "        }\n",
    "        self._order = [\n",
    "            'Underweight','Normal','Overweight','Obese',\n",
    "            'Severely Obese','Morbidly Obese','Unknown'\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.col not in X.columns:\n",
    "            if self.verbose:\n",
    "                print(f\"[WeightGrouper] '{self.col}' not found â€” skipping.\")\n",
    "            return X\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Original weight groups:\", pd.Series(X[self.col]).unique())\n",
    "\n",
    "        # Create the new grouped column but keep the original\n",
    "        X[self.new_col] = pd.Series(X[self.col]).map(self._weight_map).fillna(\"Unknown\")\n",
    "        X[self.new_col] = X[self.new_col].astype(\n",
    "            pd.api.types.CategoricalDtype(self._order, ordered=True)\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Organized weight groups:\", pd.Series(X[self.new_col]).unique())\n",
    "        return X\n",
    "\n",
    "\n",
    "# Example pipeline (keeps original \"weight\")\n",
    "weight_grouper_pipeline = Pipeline([\n",
    "    (\"weight_grouper\", WeightGrouper(verbose=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1d376031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_age_group=True, add_race_group=True):\n",
    "        self.add_age_group = add_age_group\n",
    "        self.add_race_group = add_race_group\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.add_age_group:\n",
    "            # Add new feature: age group based on 'age' column\n",
    "            X['age_group'] = X['age'].apply(map_age_category)\n",
    "            X = X.drop(columns=['age'], errors='ignore')\n",
    "\n",
    "        if self.add_race_group:\n",
    "            # Add new feature: race group based on 'race' column\n",
    "            X['race_group'] = X['race'].replace('?', 'Other')\n",
    "            X = X.drop(columns=['race'], errors='ignore')\n",
    "\n",
    "        return X\n",
    "\n",
    "feature_adder_pipeline = Pipeline([\n",
    "    (\"feature_adder\", FeatureAdder(add_age_group=True, add_race_group=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422b44a",
   "metadata": {},
   "source": [
    "## Pipeline for categorical attributes \n",
    "\n",
    "- for a better understanding and visualization for the model categorical values were preprocessed into binary coding assigning new columns for multiple categorical variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "db1e6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a pipeline to handle the categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # Other options for categorical features could be 'constant' or 'drop'\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse_output=False)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "494c928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_selector = make_column_selector(dtype_include=[\"int\", \"float\"])      # picks Age, Fare, ...\n",
    "cat_selector = make_column_selector(dtype_include=[\"object\", \"category\", \"bool\"])\n",
    "drop_columns = [\"patient_nbr\", \"encounter_id\", \"payer_code\",\n",
    "                \"medical_specialty\", \n",
    "                \"max_glu_serum\",\n",
    "                \"A1Cresult\",\n",
    "                \"diag_1\", \"diag_2\", \"diag_3\"]\n",
    "\n",
    "drop_pipelines = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"drop\", \"drop\", drop_columns)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Keep all other columns not specified in the transformers\n",
    ")\n",
    "\n",
    "transformer_pipeline = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", StandardScaler(), num_selector),\n",
    "        (\"cat\", cat_pipeline, cat_selector),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Combine all preprocessing steps into a single pipeline\n",
    "full_preprocess_pipeline = Pipeline([\n",
    "    (\"weight_grouper\", weight_grouper_pipeline),\n",
    "    (\"feature_adder\", feature_adder_pipeline),\n",
    "    (\"feature_remover\", drop_pipelines),\n",
    "    (\"transformer\", transformer_pipeline)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eddd3f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('weight_grouper',\n",
      "                 Pipeline(steps=[('weight_grouper',\n",
      "                                  WeightGrouper(verbose=True))])),\n",
      "                ('feature_adder',\n",
      "                 Pipeline(steps=[('feature_adder', FeatureAdder())])),\n",
      "                ('feature_remover',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('drop', 'drop',\n",
      "                                                  ['patient_nbr',\n",
      "                                                   'encounter_id', 'payer_code',\n",
      "                                                   'medical_specialty',\n",
      "                                                   'max_glu_serum', 'A1Cresult',...\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('num', StandardScaler(),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x28c502b60>),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('cat_encoder',\n",
      "                                                                   OneHotEncoder(sparse_output=False))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x28c5008b0>)]))])\n"
     ]
    }
   ],
   "source": [
    "print(full_preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9460dd",
   "metadata": {},
   "source": [
    "## Model Set up\n",
    "\n",
    "### Choosing the targeted value:\n",
    "\n",
    "- The ML model will require its inputs and we needed to choose a target variable for the model prediction, in this case I have decided to allow the model to predict the time a patient will be in the hospital, represented in the database as time_in_hospital \n",
    "\n",
    "- It can be important since when having a patient entering under the same conditions doctors and nurses can have an approximation of the days the patience will be within the hospital, plus it is important from an economical point of view for the expenses associated to the stay either for the member or the insurance company covering the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "33f7755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the target variable and drop it from the training and test data\n",
    "y_train = diabetes_train[\"time_in_hospital\"]  # Target variable\n",
    "X_train = diabetes_train.drop(columns=[\"time_in_hospital\"])  # Drop the target variable from the training data\n",
    "\n",
    "y_test = diabetes_test[\"time_in_hospital\"]  # Target variable for the test set\n",
    "X_test = diabetes_test.drop(columns=[\"time_in_hospital\"])  # Drop the target variable from the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "de4bf191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weight groups: ['?' '[50-75)' '[125-150)' '[75-100)' '[0-25)' '[150-175)' '[100-125)'\n",
      " '[25-50)' '[175-200)' '>200']\n",
      "Organized weight groups: ['Normal', 'Obese', 'Underweight', 'Overweight', 'Severely Obese', 'Unknown']\n",
      "Categories (7, object): ['Underweight' < 'Normal' < 'Overweight' < 'Obese' < 'Severely Obese' < 'Morbidly Obese' < 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "X_train = full_preprocess_pipeline.fit_transform(X_train)  # Apply the transformations to the training set\n",
    "\n",
    "# Get the column names of the transformed DataFrame, split by '__', and extract the last entries\n",
    "X_train.columns = X_train.columns.str.split(\"__\").str[-1]  # Flatten the column names\n",
    "# Convert the columns to a list\n",
    "columns = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "df739e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weight groups: ['?' '[100-125)' '[50-75)' '[75-100)' '[125-150)' '[25-50)' '[0-25)'\n",
      " '>200' '[175-200)' '[150-175)']\n",
      "Organized weight groups: ['Normal', 'Overweight', 'Obese', 'Underweight', 'Unknown', 'Severely Obese']\n",
      "Categories (7, object): ['Underweight' < 'Normal' < 'Overweight' < 'Obese' < 'Severely Obese' < 'Morbidly Obese' < 'Unknown']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_test = full_preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db89335",
   "metadata": {},
   "source": [
    "## Training Model Selection\n",
    "\n",
    "Now I need to select our training model \n",
    "- From all the possible candidates form classification algorithms I need to select the most suitable for my dataset\n",
    "- See how they fit the training data and evaluate their performace using cross-validation\n",
    "- Compare their mean and spread using boxplots to compare stability and accuracy and select the best performing method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "162e861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#Cross-validation function \n",
    "def evaluate_model(model, X, y, scoring=\"accuracy\", cv=5, n_repeats=10):\n",
    "    '''\n",
    "    Cross-validation.\n",
    "    Parameters:\n",
    "    - model: The machine learning model to evaluate.\n",
    "    - X: Features for training.\n",
    "    - y: Target variable for training.\n",
    "    - scoring: Scoring metric to use (default is accuracy).\n",
    "    - cv: Number of cross-validation folds (default is 5).\n",
    "    - n_repeats: Number of times to repeat the cross-validation (default is 10).\n",
    "    Returns:\n",
    "    - scores: Cross-validation scores.\n",
    "    '''\n",
    "    all_scores = []\n",
    "    # Using RepeatedStratifiedKFold to ensure that each fold has the same proportion of classes\n",
    "    # and to repeat the cross-validation process multiple times\n",
    "\n",
    "    tskf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=n_repeats, random_state=42)\n",
    "    for train_index, test_index in tskf.split(X, y):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_test_fold)\n",
    "\n",
    "        score = accuracy_score(y_test_fold, y_pred)\n",
    "        all_scores.append(score)\n",
    "\n",
    "    return np.array(all_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c83f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels to ensure consistency across folds\n",
    "le = LabelEncoder()\n",
    "#y_train_enc = le.fit_transform(y_train)\n",
    "y_train_enc = pd.Series(le.fit_transform(y_train), index=y_train.index)\n",
    "\n",
    "\n",
    "# Define a list of models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"Neural Network\": MLPClassifier(\n",
    "        random_state=42, \n",
    "        max_iter=5000, \n",
    "        hidden_layer_sizes=(20, 10), \n",
    "        activation='relu', \n",
    "        solver='adam'\n",
    "    ),\n",
    "    \"KNN Classifier\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "model_scores = {}\n",
    "for model_name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train_enc, scoring=\"accuracy\", cv=5, n_repeats=10)\n",
    "    model_scores[model_name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ad195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
